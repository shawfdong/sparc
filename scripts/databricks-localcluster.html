<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>databricks-localcluster-1.3 - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html","displayName":"Databricks Guide","icon":"question"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html","displayName":"Application Examples","icon":"code"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/courses/index.html","displayName":"Training","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":false,"nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-35f30dcf90b59210c3f253a98dbb4d255fa4a1773385163b3cec143c1271e065","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-876320751f9b6e4318a1235c9d5f862196d02008ff478f483e2820c48c59d1fe","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":8,"memory-optimized":1,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.31","accountsLimit":3,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"useFixedStaticNotebookVersionForDevelopment":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAclService":true,"docsDomain":"https://docs.cloud.databricks.com/","enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"d021cfb95994095f8eba80e12423a5f1461121b6","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/A%20Gentle%20Introduction%20to%20Apache%20Spark%20on%20Databricks.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/Quick%20Start%20DataFrames.html","displayName":"Quick Start DataFrames","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/GSW%20Passing%20Analysis%20(new).html","displayName":"GSW Passing Analysis (new)","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":657385009570686,"name":"databricks-localcluster-1.3","language":"scala","commands":[{"version":"CommandV1","origId":657385009570688,"guid":"ab2f6115-ad61-4090-abec-6d2b997cead8","subtype":"command","commandType":"auto","position":1.0,"command":"/* LocalCluster.scala \n    By Zhong Wang, Xiandong Meng\n    version 1.3.1\n    2016-10-28 added function to dump local graph to disk, then read it from disk for cc\n    2016-10-05 added code to force exit at the end of program and print the version number  \n    2016-09-28 added random sample down abundant k-mers\n    2016-09-15 added elapsed time for each step; added k-mer histogram\n    2016-09-09 added k-mer normalization to remove abundant k-mers to improve parallelism\n    2016-09-08 added repartition to improve parallelism\n    2016-09-01 improved program logistics\n    2016-05-27 added config parameter check\n    \n    2016-05-18 hash kmers\n               require input file conventions to bypass AWS s3 has no directory structure, files should be named \"sample##.seq\" \n    2016-04-28 changed to a weighted graph, no need of intermidiate step to save/load edge file\n    2016-04-26 \n    \n*/\n\nimport org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf  \nimport org.apache.spark.rdd.RDD    \nimport scala.util.hashing.{MurmurHash3=>MH3}   \nimport java.nio.file.{Files, Paths}\nimport scala.collection.JavaConversions._\nimport scala.io.Source\nimport scala.util.Random","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968583E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5b4e8b22-b37d-437b-a13a-82c9b1224414"},{"version":"CommandV1","origId":657385009570689,"guid":"2fa55d49-b499-43ef-af9b-a964de20e7bd","subtype":"command","commandType":"auto","position":2.0,"command":"val AccessKey = \"\"\nval SecretKey = \"\"\nval EncodedSecretKey = SecretKey.replace(\"/\", \"%2F\")\nval AwsBucketName = \"sparkAssembler\"\nval MountName = \"rawdata\"\ndbutils.fs.unmount(s\"/mnt/$MountName\")\ndbutils.fs.mount(s\"s3a://$AccessKey:$EncodedSecretKey@$AwsBucketName\", s\"/mnt/$MountName\")","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968584E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f1a457c7-70e7-4ffd-bc49-288d06d84bff"},{"version":"CommandV1","origId":657385009570690,"guid":"c71576f1-0710-4e5f-8872-c4f43bee2600","subtype":"command","commandType":"auto","position":3.0,"command":"// input and output specification \nvar input_dir = \"/mnt/rawdata/old_samples\" \nvar sample_no = 3 // number of samples \n// default parameters \n// var input_dir = \"s3://sparkAssembler\"    // S3 or local path\nvar output_prefix = \"test_\" // output dir prefix\n\n/* small datasets, 1gb\nvar input_dir = \"/mnt/rawdata/big_samples/cow_1gb\" \nvar sample_no = 1 // number of samples \n// default parameters \n// var input_dir = \"s3://sparkAssembler\"    // S3 or local path\nvar output_prefix = \"small_\" // output dir prefix\n*/\n\n/* medium dataset 1, 10gb\nvar input_dir = \"/mnt/rawdata/big_samples/cow_rumen\" \nvar sample_no = 1 // number of samples \n// default parameters \n// var input_dir = \"s3://sparkAssembler\"    // S3 or local path\nvar output_prefix = \"medium1_\" // output dir prefix\n*/\n\n/* medium dataset 2, 50gb\nvar input_dir = \"/mnt/rawdata/big_samples/cow_rumen\" \nvar sample_no = 4 // number of samples \n// default parameters \n// var input_dir = \"s3://sparkAssembler\"    // S3 or local path\nvar output_prefix = \"medium2_\" // output dir prefix\n*/","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968588E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c06683dc-c5cf-4700-bc51-eab6f1c9eeac"},{"version":"CommandV1","origId":657385009570691,"guid":"c6a67177-48a7-4cf9-86fc-4bad24018c7a","subtype":"command","commandType":"auto","position":4.0,"command":"        var temp_dir = \"none\" // directory to store temporary files, none(default no saving), HDFS, local, or S3       \n        var mp1 = 24 // parallelism multiplier, change the default partition size to smaller ones\n        var mp2 = 480 // parallelism multiplier, change the default partition size to smaller ones\n        var mp3 = 1 // parallelism multiplier, change the default partition size to smaller ones    \n        var contamination = 0.00005 // remove most abundant k-mers, likely due to contamination    \n        // default run-time parameters, TODO: need to be learned from data            \n        var k = 31\n        var step = 1    // number of k-mer to step over, increase the number to reduce data size and achieve heuristics\n        var min_kmer_count = 2\n        var max_kmer_count = 200\n        var min_shared_kmers = 2\n        var max_shared_kmers = 5000\n        val seed = 1234 // hash seed\n        var min_edge_weight = 100    // this option is not used\n        var min_reads_per_cluster = 10 \n            \n        var debug = false  // enable debugging output\n        var out_singletons = false //enable singleton output    ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968591E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"aa91e057-525b-4e9f-8716-46d13a23da89"},{"version":"CommandV1","origId":657385009570692,"guid":"99dd88b3-68e2-45c8-a28c-dab033d905e4","subtype":"command","commandType":"auto","position":5.0,"command":"        // define a map for reverse complement a sequence\n        val rc = Map('A' -> 'T', 'T' -> 'A', 'C' -> 'G', 'G' -> 'C', 'N' -> 'N')\n        // all kmers from a string   \n        def emitKmers(s: String, k: Int) = { for {start <- 0 to (s.length - k) by step} yield s.substring(start, start+k) }\n        def generate_kmer(k:Int, seqID:Long, seq:String) ={\n            /***\n            emit k-mers of length k in sequence\n            k-mer containing N is omitted\n            output key: hashed(k-mer), value: list of readIDs \n            ***/ \n\n            val seqrc = seq.reverse.map(rc(_))\n            val both = seq + \"N\" + seqrc \n            emitKmers(both, k).filter(x => !x.contains('N')).map{\n                x => MH3.stringHash(x, seed).toLong -> List(seqID) // both direction \n            }        \n        }\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968594E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2e96efde-f3bb-43ec-8a00-e8c975f11ec5"},{"version":"CommandV1","origId":657385009570693,"guid":"8f1ccc83-e187-4a78-b0d8-55978b1b4954","subtype":"command","commandType":"auto","position":6.0,"command":"//val conf = new SparkConf().setAppName(\"SparkLC\")\n//val sc = new SparkContext(conf)\n        // read seq file, Hash readID to integer \n        // read all sample files\n        // determine the partition size\n        println(new java.util.Date(System.currentTimeMillis) + \": Program started ...\" )\n        val start = System.currentTimeMillis    \n        val seqFiles = (0 to sample_no-1).map(x => input_dir + \"/sample\" + x.toString + \".seq\")\n        \n        // select abundant k-mers (predicting from 10,000 reads)\n        val no_lines_to_read = 100000    \n        val smallReadsRDD = sc.parallelize(sc.textFile(seqFiles.mkString(\",\")).take(no_lines_to_read), 8).map {\n            line => line.split(\"\\t\")}.map{ x => (MH3.stringHash(x(0)).toLong, x(1))\n        }\n        val smallKmersRDD = smallReadsRDD.map(x => generate_kmer(k, x._1, x._2)).flatMap(x => x).reduceByKey{\n            (x,y) => x++y\n        }.map(x => (x._1, x._2.length))\n        // take top 0.005% kmers    \n        val topN = (smallKmersRDD.count * contamination).toInt    \n        val topKmers = smallKmersRDD.sortBy(_._2,false).collect().take(topN)\n        // Broadcast this to all nodes    \n        val topKmersB = sc.broadcast(sc.parallelize(topKmers).collectAsMap)    \n        val totalTime1 = System.currentTimeMillis   \n        println(\"Contamination prediction time: %.2f minutes\".format((totalTime1-start).toFloat/60000))      ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968598E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ebf3710d-005f-4ab6-a119-0e5c4e309c38"},{"version":"CommandV1","origId":657385009570694,"guid":"0458048d-3afa-44a2-8e8b-6d15be66f131","subtype":"command","commandType":"auto","position":7.0,"command":"        val p = sc.textFile(seqFiles.mkString(\",\")).getNumPartitions    \n        val allReadsRDD = sc.textFile(seqFiles.mkString(\",\"), mp1*p).map{\n            line => line.split(\"\\t\")}.map{ x => (MH3.stringHash(x(0)).toLong, x(1))\n        }\n        val kmersRDD = allReadsRDD.map(x => generate_kmer(k, x._1, x._2)).flatMap(x => x).filter{\n            x => topKmersB.value.getOrElse(x._1, -1)<0\n        }.reduceByKey(_.distinct++_.distinct, numPartitions = mp2*p)\n            \n        // generate k-mer frequencies    \n        val (bucket, size) = kmersRDD.map(x => x._2.length).histogram(100)    \n        for{ i<-0 to 99} println(\"%.1f %d\".format(bucket(i), size(i)))    \n            \n        // subsampling very abundant k-mers that appear in many reads\n        // remove very rare k-mers that appear only in one read: likely due to sequencing error\n        val filteredKmersRDD = kmersRDD.filter{\n            x => (x._2.length >= min_kmer_count)   \n        }.map{ \n            x => if(x._2.length <= max_kmer_count) x else (x._1, Random.shuffle(x._2).take(max_kmer_count))\n        }\n        val total_filtered_kmers =  filteredKmersRDD.count() \n        val formatter = java.text.NumberFormat.getIntegerInstance    \n        println(\"Total filtered kmers: \" + formatter.format(total_filtered_kmers) )\n        val totalTime2 = System.currentTimeMillis \n        println(\"K-mer generation and filter time: %.2f minutes\".format((totalTime2-totalTime1).toFloat/60000))\n\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968601E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"adf62e99-3e0a-4c19-8a1c-222b40766dd5"},{"version":"CommandV1","origId":657385009570695,"guid":"306a4c23-5947-4951-b506-863c21c5eb94","subtype":"command","commandType":"auto","position":7.5,"command":"       // local similarity\n\n        // obtain pairs of reads that share at least one k-mer, this is the edges \n        val localReadPairsRDD = filteredKmersRDD.flatMap(x => {\n            (2 to 2).flatMap(x._2.sorted.combinations).map(x => Edge(x(0), x(1), 1.0))\n        })\n        val localReadGraph = Graph(allReadsRDD, localReadPairsRDD).groupEdges(_ + _)\n        // only keep read pairs with significant overlap   \n        val filteredReadPairs = localReadGraph.edges\n            .filter { \n            case Edge(src, dst, count) => count >= min_shared_kmers  \n        }.map{ case Edge(src, dst, count) => (src, dst)}\n        println(new java.util.Date(System.currentTimeMillis) + \": Compute local similarity ...\" )    \n        val total_local_pairs =  filteredReadPairs.count()   \n        println(\"Total filtered overlapping pairs: \" + formatter.format(total_local_pairs) )\n            \n        val totalTime3 = System.currentTimeMillis   \n        println(\"Read graph generation and filter time: %.2f minutes\".format((totalTime3-totalTime2).toFloat/60000))    ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968605E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"42a07a95-0f41-473c-be52-d9d79d1fba9e"},{"version":"CommandV1","origId":657385009570696,"guid":"389bb8ec-4244-40e9-9e18-df4c1ab32900","subtype":"command","commandType":"auto","position":7.75,"command":"        // repartition the data into smaller number of partitions to improve connected-component performance     \n        val filteredGraph = {\n            if(temp_dir == \"none\") {\n                val finalReadPairs =  filteredReadPairs.coalesce(numPartitions=p*mp3, shuffle=false)   \n                Graph.fromEdgeTuples(finalReadPairs, 1)  \n            } else { \n                // save edges into a file then read it\n                var tempEdgeFile = output_prefix + \"_edges.txt\" // S3 \n                if(temp_dir == \"HDFS\") { \n                    tempEdgeFile = \"hdfs:///\" + output_prefix.split('/').last + \"_\" + k.toString() + \"_edges\" \n                }  \n                if(temp_dir == \"local\") { \n                    tempEdgeFile = output_prefix.split('/').last + \"_\" + k.toString() + \"_edges\" \n                } \n\n                filteredReadPairs.coalesce(p*mp3).saveAsTextFile(tempEdgeFile)      \n                def toEdge(l:String) = {\n                  val e = l.drop(1).dropRight(1).split(',')\n                  (e(0).toLong, e(1).toLong)\n                }\n                val finalReadPairs = sc.textFile(tempEdgeFile).map(toEdge)\n\n                Graph.fromEdgeTuples(finalReadPairs, 1L) \n            } \n        }   ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478103243861E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"histogram","width":"260","height":"auto","xColumns":[],"yColumns":["count"],"pivotColumns":[],"pivotAggregation":"sum","customPlotOptions":{"histogram":[{"key":"bins","value":"100"}],"quantilePlot":[],"boxPlot":[],"lineChart":[]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a3699175-43d8-4f43-b1f0-3b62d7ab4c7c"},{"version":"CommandV1","origId":1201512784400297,"guid":"dfc649fb-3db0-47d7-8008-654766f4a535","subtype":"command","commandType":"auto","position":7.765625,"command":"        // graph with local edges       \n        println(new java.util.Date(System.currentTimeMillis) + \": Start clustering using connected components ...\" )          \n        val cc_vertices = filteredGraph.connectedComponents().vertices   \n        val connectedReadsRDD = allReadsRDD.join(cc_vertices).map { \n            case (id, (seq, index)) => (index, (id, seq)) \n        }.groupByKey().filter{\n            x => x._2.size >= min_reads_per_cluster\n        }\n            \n        val large_clusters = connectedReadsRDD.count \n        println(f\"Total clusters: $large_clusters%d have $min_reads_per_cluster%d reads or more.\" )\n        val totalTime4 = System.currentTimeMillis   \n        println(\"Connected Component computation time: %.2f minutes\".format((totalTime4-totalTime3).toFloat/60000))","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478103253419E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5fee01f7-3bc4-4e44-b6aa-de348f617a16"},{"version":"CommandV1","origId":657385009570700,"guid":"25cc2765-899a-4e00-8a81-63cdc660610e","subtype":"command","commandType":"auto","position":7.796875,"command":"        println( new java.util.Date(System.currentTimeMillis) + \": Saving final results ...\" )    \n        val clusterFile = output_prefix + \"AllSamples_\" + k.toString() + \"_clusters\"  \n        connectedReadsRDD.coalesce(1).saveAsTextFile(clusterFile)\n            \n        if(out_singletons) {\n            val singletonReadsRDD = allReadsRDD.subtractByKey(cc_vertices)    \n            val singletonFile = output_prefix + \"AllSamples_\" + k.toString() + \"_singletons\"  \n            singletonReadsRDD.coalesce(1).saveAsTextFile(singletonFile)\n        }   \n        \n        println(new java.util.Date(System.currentTimeMillis) + \": Job finished.\" )\n        val totalTime = System.currentTimeMillis  \n        println(\"Total elapsed time: %.2f minutes\".format((totalTime-start).toFloat/60000)) \n        sys.exit(0) ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.478102968614E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b3a2a2c8-9d5f-47b1-a23a-787d01424ed2"}],"dashboards":[],"guid":"b41b49a0-bec0-4468-a9f5-c08897d1568a","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
